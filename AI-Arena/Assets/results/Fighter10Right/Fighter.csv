Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,2.0273716,139.6,-1.1116303,-2.634902128336772,-2.634902128336772,0.4686618,0.026844272,0.00028455182,0.19485056,0.004743045,1.0
100000,1.7402813,97.58811881188119,-0.38733572,0.07297494873522299,0.07297494873522299,0.13899633,0.023936933,0.0002567831,0.18559437,0.0042811586,1.0
150000,1.5882481,80.98366013071896,0.20219672,0.5999650758509745,0.5999650758509745,0.053578895,0.021324057,0.00022595198,0.17531732,0.0037683342,1.0
200000,1.4397011,66.46558704453442,0.50318897,0.8238625271080157,0.8238625271080157,0.028805306,0.024226734,0.00019511646,0.1650388,0.0032554362,1.0
250000,1.3012195,63.09987195902689,0.6699054,0.9220260300037956,0.9220260300037956,0.012438515,0.024088103,0.00016420915,0.15473635,0.002741344,1.0
300000,1.1776944,58.591179976162095,0.70806605,0.9245518424744685,0.9245518424744685,0.013347114,0.025127277,0.00013331769,0.14443919,0.0022275164,1.0
350000,1.1017736,58.76498800959233,0.7087936,0.9373173805301269,0.9373173805301269,0.012797574,0.024065912,0.00010253142,0.13417712,0.0017154381,1.0
400000,1.0902721,58.61309523809524,0.69916254,0.9456471959761698,0.9456471959761698,0.013483373,0.021465562,7.479804e-05,0.12493266,0.0012541391,1.0
450000,1.0791543,58.778443113772454,0.7177859,0.9380382741288991,0.9380382741288991,0.013826626,0.023667146,4.7067086e-05,0.115688995,0.0007928812,1.0
500000,1.0545303,57.198375870069604,0.7100632,0.9504018548992869,0.9504018548992869,0.010676308,0.024067929,1.6250377e-05,0.10541676,0.00028029626,1.0
