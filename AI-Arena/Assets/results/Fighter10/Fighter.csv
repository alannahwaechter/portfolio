Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,2.0209284,174.29929577464787,-1.6736363,-2.8871343484701253,-2.8871343484701253,0.36159703,0.023876242,0.00028456096,0.19485363,0.004743197,1.0
100000,1.7735564,99.01397205588822,-0.8241662,0.0006766054052043246,0.0006766054052043246,0.2559534,0.026713971,0.00025680687,0.18560229,0.0042815534,1.0
150000,1.5671456,82.89949748743719,-0.007365913,0.5234325989180764,0.5234325989180764,0.08034297,0.023613777,0.00022589238,0.17529742,0.0037673425,1.0
200000,1.4577099,68.81398601398601,0.48218715,0.7654999942003682,0.7654999942003682,0.036382206,0.023151396,0.00019508583,0.16502862,0.003254927,1.0
250000,1.3924023,65.40903054448872,0.6425189,0.8899070328648189,0.8899070328648189,0.0163325,0.02332643,0.00016428115,0.15476035,0.002742542,1.0
300000,1.2590905,59.23373493975904,0.70962673,0.9382215494092984,0.9382215494092984,0.008219423,0.023411794,0.00013348015,0.14449336,0.0022302186,1.0
350000,1.2107255,59.123353293413174,0.70751464,0.9191343718343981,0.9191343718343981,0.014423314,0.022604862,0.000102641374,0.13421378,0.0017172666,1.0
400000,1.1472474,59.09915356711004,0.6981202,0.9116867421262236,0.9116867421262236,0.015088436,0.024337497,7.487694e-05,0.12495895,0.0012554516,1.0
450000,1.1279545,60.495085995085994,0.69397587,0.9368016316389745,0.9368016316389745,0.012918134,0.023384843,4.715517e-05,0.115718365,0.00079434615,1.0
500000,1.0945145,58.49403341288783,0.71197635,0.940166464357396,0.940166464357396,0.010421532,0.02370016,1.63681e-05,0.10545601,0.00028225442,1.0
